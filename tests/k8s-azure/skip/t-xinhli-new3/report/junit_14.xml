<?xml version="1.0" encoding="UTF-8"?>
  <testsuite tests="10" failures="1" time="283.118593069">
      <testcase name="[sig-storage] Volume Provisioning On Clustered Datastore [Feature:vsphere] verify dynamic provision with default parameter on clustered datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl api-versions should check if v1 is in available api versions  [Conformance]" classname="Kubernetes e2e suite" time="7.6079202630000005"></testcase>
      <testcase name="[sig-network] ESIPP [Slow] [DisabledForLargeClusters] should work for type=NodePort" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] [Feature:BootstrapTokens] should delete the signed bootstrap tokens from clusterInfo ConfigMap when bootstrap token is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Network Partition [Disruptive] [Slow] [k8s.io] [ReplicationController] should eagerly create replacement pod during network partition when termination grace is non-zero" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale up with two External metrics from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects NO client request should support a client that connects, sends DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PVC Protection Verify that scheduling of a pod that uses PVC that is being deleted fails and the pod becomes Unschedulable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl logs should be able to retrieve and filter logs  [Conformance]" classname="Kubernetes e2e suite" time="42.211341946"></testcase>
      <testcase name="[sig-storage] Dynamic Provisioning [k8s.io] GlusterDynamicProvisioner should create and delete persistent volumes [fast]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Provisioning on Datastore [Feature:vsphere] verify dynamically provisioned pv using storageclass fails on an invalid datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="34.44365894"></testcase>
      <testcase name="[sig-storage] Subpath [Volume type: nfsPVC] should support non-existent path" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should provide basic identity" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should remove from active list jobs that have been deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support seccomp default which is unconfined [Feature:Seccomp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath [Volume type: nfs] should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Update Demo should create and stop a replication controller  [Conformance]" classname="Kubernetes e2e suite" time="38.316261709"></testcase>
      <testcase name="[sig-storage] Projected should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="13.365752205"></testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using {extensions Deployment} with 0 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] Cluster level logging using Elasticsearch [Feature:Elasticsearch] should check that logs from containers are ingested into Elasticsearch" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: blockfs] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [Feature:Example] [k8s.io] Secret should create a pod that reads a secret" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath [Volume type: emptyDir] should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Proxy server should support --unix-socket=/path  [Conformance]" classname="Kubernetes e2e suite" time="9.203661857"></testcase>
      <testcase name="[sig-storage] Subpath [Volume type: gcePDPVC] should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected should provide podname as non-root with fsgroup and defaultMode [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath [Volume type: gcePDPartitioned] should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Load capacity [Feature:ManualPerformance] should be able to handle 30 pods per node { Random} with 0 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should provision storage [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]" classname="Kubernetes e2e suite" time="18.64856739"></testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should return command exit codes" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should not scale GPU pool up if pod does not require GPUs [GpuType:nvidia-tesla-v100] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volumes ConfigMap should be mountable" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] DNS configMap nameserver Forward external name lookup should forward externalname lookup to upstream nameserver [Slow][Serial]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects a client request should support a client that connects, sends DATA, and disconnects" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicaSet should serve a basic image on each replica with a private image" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should scale down GPU pool from 1 [GpuType:nvidia-tesla-k80] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Garbage collector should delete jobs and pods created by cronjob" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath [Volume type: hostPath] should support existing single file" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath [Volume type: hostPath] should unmount if pod is gracefully deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPreemption [Serial] [Feature:PodPreemption] validates lower priority pod preemption by critical pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should be able to scale down by draining multiple pods one by one as dictated by pdb[Feature:ClusterSizeAutoscalingScaleDown]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] [Feature:GPUDevicePlugin] run Nvidia GPU Device Plugin tests on Container Optimized OS only" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Dynamic Provisioning DynamicProvisioner should test that deleting a claim before the volume is provisioned deletes the volume." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl taint [Serial] should remove all the taints with the same key off a node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set fsGroup for one pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume plugin streaming [Slow] Ceph-RBD [Feature:Volumes] should write files of various sizes, verify size, validate content" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="28.537452696"></testcase>
      <testcase name="[k8s.io] [Feature:Example] [k8s.io] Cassandra should create and scale cassandra" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="23.768431944"></testcase>
      <testcase name="[sig-storage] Subpath [Volume type: emptyDir] should support existing directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Simple pod should support exec through an HTTP proxy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] MetricsGrabber should grab all metrics from a Scheduler." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]" classname="Kubernetes e2e suite" time="66.752674646">
          <failure type="Failure">/home/t-xinhli/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc421a1ad00&gt;: {&#xA;        s: &#34;Namespace e2e-tests-configmap-fx8qd is active&#34;,&#xA;    }&#xA;    Namespace e2e-tests-configmap-fx8qd is active&#xA;not to have occurred&#xA;/home/t-xinhli/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:88</failure>
          <system-out>[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /home/t-xinhli/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;Jun 13 03:18:55.825: INFO: &gt;&gt;&gt; kubeConfig: /home/t-xinhli/new-provider/cloud-provider-azure/t-xinhli-new3/kubeconfig&#xA;�[1mSTEP�[0m: Building a namespace api object&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /home/t-xinhli/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79&#xA;Jun 13 03:18:55.972: INFO: Waiting up to 1m0s for all nodes to be ready&#xA;Jun 13 03:19:56.019: INFO: Waiting for terminating namespaces to be deleted...&#xA;Jun 13 03:19:56.021: INFO: Unexpected error occurred: Namespace e2e-tests-configmap-fx8qd is active&#xA;[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /home/t-xinhli/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;e2e-tests-sched-pred-7tktn&#34;.&#xA;�[1mSTEP�[0m: Found 0 events.&#xA;Jun 13 03:19:56.036: INFO: POD                                                         NODE                       PHASE      GRACE  CONDITIONS&#xA;Jun 13 03:19:56.036: INFO: pod-configmaps-8c9965ff-6eb8-11e8-b22e-000d3a184d46         k8s-agentpool1-31296431-0  Pending           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:19 +0000 UTC ContainersNotReady containers with unready status: [configmap-volume-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:14 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: pod-configmaps-8bd86822-6eb8-11e8-baab-000d3a184d46         k8s-agentpool1-31296431-0  Succeeded         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:19 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:19 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:13 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: pod-configmaps-9bcf0289-6eb8-11e8-a315-000d3a184d46         k8s-agentpool1-31296431-0  Pending           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:40 +0000 UTC ContainersNotReady containers with unready status: [delcm-volume-test updcm-volume-test createcm-volume-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:40 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: liveness-http                                               k8s-agentpool1-31296431-1  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:16:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:16:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:16:08 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: liveness-http                                               k8s-agentpool1-31296431-0  Pending           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:19 +0000 UTC ContainersNotReady containers with unready status: [liveness]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:15 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: daemon-set-rgv4s                                            k8s-agentpool1-31296431-0  Pending           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:37 +0000 UTC ContainersNotReady containers with unready status: [app]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:37 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: daemon-set-v6w87                                            k8s-agentpool1-31296431-1  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:39 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: frontend-77c5d665c7-5fkvl                                   k8s-agentpool1-31296431-1  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:44 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:08 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: frontend-77c5d665c7-9hgkb                                   k8s-agentpool1-31296431-0  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:08 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: frontend-77c5d665c7-ls5tt                                   k8s-agentpool1-31296431-0  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:08 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: redis-master-6c74995565-8zl9q                               k8s-agentpool1-31296431-0  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: redis-slave-597bcc5997-7xp5n                                k8s-agentpool1-31296431-0  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: redis-slave-597bcc5997-k94w4                                k8s-agentpool1-31296431-1  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: netserver-0                                                 k8s-agentpool1-31296431-0  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: netserver-1                                                 k8s-agentpool1-31296431-1  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: downwardapi-volume-8f51369e-6eb8-11e8-a5be-000d3a184d46     k8s-agentpool1-31296431-0  Pending           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:19 +0000 UTC ContainersNotReady containers with unready status: [client-container]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:19 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: pod-projected-secrets-8070ab60-6eb8-11e8-8891-000d3a184d46  k8s-agentpool1-31296431-1  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:54 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: downwardapi-volume-8afd8ffc-6eb8-11e8-a995-000d3a184d46     k8s-agentpool1-31296431-0  Succeeded         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:12 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:12 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:11 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: pod-secrets-727f66e9-6eb8-11e8-addd-000d3a184d46            k8s-agentpool1-31296431-1  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:30 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: cloud-controller-manager-k8s-master-31296431-0              k8s-master-31296431-0      Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: heapster-6f7b9bdf-52ntm                                     k8s-agentpool1-31296431-0  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:58:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:42 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: kube-addon-manager-k8s-master-31296431-0                    k8s-master-31296431-0      Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: kube-apiserver-k8s-master-31296431-0                        k8s-master-31296431-0      Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: kube-controller-manager-k8s-master-31296431-0               k8s-master-31296431-0      Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: kube-dns-v20-59b4f7dc55-67fbv                               k8s-agentpool1-31296431-0  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:58:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:41 +0000 UTC  }]&#xA;Jun 13 03:19:56.036: INFO: kube-dns-v20-59b4f7dc55-phwv9                               k8s-agentpool1-31296431-0  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:58:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:41 +0000 UTC  }]&#xA;Jun 13 03:19:56.037: INFO: kube-proxy-5976x                                            k8s-master-31296431-0      Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:26 +0000 UTC  }]&#xA;Jun 13 03:19:56.037: INFO: kube-proxy-89kmg                                            k8s-agentpool1-31296431-0  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:26 +0000 UTC  }]&#xA;Jun 13 03:19:56.037: INFO: kube-proxy-fr77g                                            k8s-agentpool1-31296431-1  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:26 +0000 UTC  }]&#xA;Jun 13 03:19:56.037: INFO: kube-scheduler-k8s-master-31296431-0                        k8s-master-31296431-0      Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  }]&#xA;Jun 13 03:19:56.037: INFO: kubernetes-dashboard-65c8bbc84b-tf6lc                       k8s-agentpool1-31296431-0  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:42 +0000 UTC  }]&#xA;Jun 13 03:19:56.037: INFO: metrics-server-5c74cf6d4f-h5mgt                             k8s-agentpool1-31296431-0  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:40 +0000 UTC  }]&#xA;Jun 13 03:19:56.037: INFO: tiller-deploy-d85ccb55c-pk2j4                               k8s-agentpool1-31296431-0  Running           [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:58:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:42 +0000 UTC  }]&#xA;Jun 13 03:19:56.037: INFO: &#xA;Jun 13 03:19:56.040: INFO: &#xA;Logging node info for node k8s-agentpool1-31296431-0&#xA;Jun 13 03:19:56.046: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:k8s-agentpool1-31296431-0,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/k8s-agentpool1-31296431-0,UID:74eaa19b-6eb5-11e8-9654-000d3a134c0e,ResourceVersion:10962,Generation:0,CreationTimestamp:2018-06-13 02:57:06 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{agentpool: agentpool1,beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: Standard_F2,beta.kubernetes.io/os: linux,failure-domain.beta.kubernetes.io/region: eastus,failure-domain.beta.kubernetes.io/zone: 0,kubernetes.azure.com/cluster: t-xinhli-new3,kubernetes.io/hostname: k8s-agentpool1-31296431-0,kubernetes.io/role: agent,storageprofile: managed,storagetier: Standard_LRS,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:10.244.1.0/24,DoNotUse_ExternalID:,ProviderID:azure:///subscriptions/c4528d9e-c99a-48bb-b12d-fde2176a43b8/resourceGroups/t-xinhli-new3/providers/Microsoft.Compute/virtualMachines/k8s-agentpool1-31296431-0,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{31158935552 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4114132992 0} {&lt;nil&gt;} 4017708Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{28043041951 0} {&lt;nil&gt;} 28043041951 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4009275392 0} {&lt;nil&gt;} 3915308Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2018-06-13 02:58:57 +0000 UTC 2018-06-13 02:58:57 +0000 UTC RouteCreated RouteController created a route} {OutOfDisk False 2018-06-13 03:19:48 +0000 UTC 2018-06-13 02:57:05 +0000 UTC KubeletHasSufficientDisk kubelet has sufficient disk space available} {MemoryPressure False 2018-06-13 03:19:48 +0000 UTC 2018-06-13 02:57:05 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2018-06-13 03:19:48 +0000 UTC 2018-06-13 02:57:05 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2018-06-13 03:19:48 +0000 UTC 2018-06-13 02:57:05 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2018-06-13 03:19:48 +0000 UTC 2018-06-13 02:57:36 +0000 UTC KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.240.0.5} {Hostname k8s-agentpool1-31296431-0}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:79fe54da919d45ff9e81f9bf6ebd36fb,SystemUUID:24B4B64B-ECDB-7144-A474-9331C73E23DF,BootID:3b74ef78-7064-4109-a243-e752fa302910,KernelVersion:4.13.0-1018-azure,OSImage:Ubuntu 16.04.4 LTS,ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.12.0-alpha.0,KubeProxyVersion:v1.12.0-alpha.0,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcrio.azureedge.net/google_containers/hyperkube-amd64@sha256:b92fc22f40fa4bf5d72ddd6e82ffd5d5a98ac9201eeb086843899f3a9fbf3eee gcrio.azureedge.net/google_containers/hyperkube-amd64:v1.12.0-alpha.0] 619581352} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils-amd64@sha256:b7fb726f862504ca6afcc512abf30456cbf4fbd6b4589af46d2c1d27cb89acf0 gcr.io/kubernetes-e2e-test-images/jessie-dnsutils-amd64:1.0] 189192551} {[k8s.gcr.io/nginx-slim-amd64@sha256:6654db6d4028756062edac466454ee5c9cf9b20ef79e35a81e3c840031eb1e2b k8s.gcr.io/nginx-slim-amd64:0.20] 103591055} {[k8s-gcrio.azureedge.net/kubernetes-dashboard-amd64@sha256:dc4026c1b595435ef5527ca598e1e9c4343076926d7d62b365c44831395adbd0 k8s-gcrio.azureedge.net/kubernetes-dashboard-amd64:v1.8.3] 102319441} {[k8s.gcr.io/nginx-slim-amd64@sha256:8ca6a9ecef3b2ef02f6e0c3d449235d9c53d532f420cc0a29a6a133aa88df256 k8s.gcr.io/nginx-slim-amd64:0.21] 95339966} {[k8s-gcrio.azureedge.net/heapster-amd64@sha256:d4d10455d921802bdb004e7edfe423a2b2f88911319b48abf47e0af909f27f15 k8s-gcrio.azureedge.net/heapster-amd64:v1.5.1] 75318380} {[gcrio.azureedge.net/kubernetes-helm/tiller@sha256:394fb7d5f2fbaca54f6a0dec387cef926f6ae359786c89f7da67db173b97a322 gcrio.azureedge.net/kubernetes-helm/tiller:v2.8.1] 71509364} {[k8s-gcrio.azureedge.net/k8s-dns-kube-dns-amd64@sha256:6d8e0da4fb46e9ea2034a3f4cab0e095618a2ead78720c12e791342738e5f85d k8s-gcrio.azureedge.net/k8s-dns-kube-dns-amd64:1.14.8] 50456751} {[k8s-gcrio.azureedge.net/metrics-server-amd64@sha256:49a9f12f7067d11f42c803dbe61ed2c1299959ad85cb315b25ff7eef8e6b8892 k8s-gcrio.azureedge.net/metrics-server-amd64:v0.2.1] 42541759} {[k8s-gcrio.azureedge.net/k8s-dns-dnsmasq-nanny-amd64@sha256:93c827f018cf3322f1ff2aa80324a0306048b0a69bc274e423071fb0d2d29d8b k8s-gcrio.azureedge.net/k8s-dns-dnsmasq-nanny-amd64:1.14.8] 40951779} {[k8s-gcrio.azureedge.net/addon-resizer@sha256:507aa9845ecce1fdde4d61f530c802f4dc2974c700ce0db7730866e442db958d k8s-gcrio.azureedge.net/addon-resizer:1.8.1] 32968591} {[gcr.io/kubernetes-e2e-test-images/dnsutils-amd64@sha256:9e1873dbe19894688dc3d129994c952891d43794df2480217f899bf92732d35f gcr.io/kubernetes-e2e-test-images/dnsutils-amd64:1.0] 8871636} {[k8s-gcrio.azureedge.net/exechealthz-amd64@sha256:503e158c3f65ed7399f54010571c7c977ade7fe59010695f48d9650d83488c0a k8s-gcrio.azureedge.net/exechealthz-amd64:1.2] 8374840} {[gcr.io/kubernetes-e2e-test-images/netexec-amd64@sha256:2edfad424a541b9e024f26368d3a5b7dcc1d7cd27a4ee8c1d8c3f81d9209ab2e gcr.io/kubernetes-e2e-test-images/netexec-amd64:1.0] 6227659} {[gcr.io/kubernetes-e2e-test-images/redis-amd64@sha256:3e01bcaf67cb9b5c9fa7f57ba92539c8962d59c9647b91e9ec5047a89e2bc49a gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0] 5850779} {[gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64@sha256:2dd4032e98a0450d95a0ac71a5e465f542a900812d8c41bc6ca635aed1a5fc91 gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0] 5470001} {[gcr.io/kubernetes-e2e-test-images/nautilus-amd64@sha256:a4e859e40750d0e1a94470445bea3bc0ba4edc7363863a0be7a3714336040daa gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0] 4415165} {[gcr.io/kubernetes-e2e-test-images/kitten-amd64@sha256:36e05bc3dfe1cd0f2d2d119f47fbb6e1c75037b291073012d57fbf28af3de3d7 gcr.io/kubernetes-e2e-test-images/kitten-amd64:1.0] 4408701} {[gcr.io/kubernetes-e2e-test-images/test-webserver-amd64@sha256:cd237408ae94e22c4f5c6d7c6f56708341db6c428180fe1fe011c17bf9d03c50 gcr.io/kubernetes-e2e-test-images/test-webserver-amd64:1.0] 4393904} {[gcr.io/kubernetes-e2e-test-images/mounttest-amd64@sha256:dc4e2dcfbde16249c4662de673295d00778577bc2e2ca7013a1b85d4f47398ca gcr.io/kubernetes-e2e-test-images/mounttest-amd64:1.0] 1450451} {[gcr.io/kubernetes-e2e-test-images/mounttest-user-amd64@sha256:dda6519a95c934b46731a6b1492fed1b48ccc6d4aed4b754a46d7de8063a3e2b gcr.io/kubernetes-e2e-test-images/mounttest-user-amd64:1.0] 1450451} {[busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47 busybox:latest] 1146369} {[k8s-gcrio.azureedge.net/pause-amd64@sha256:59eec8837a4d942cc19a52b8c09ea75121acc38114a2c68b98983ce9356b8610 k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s-gcrio.azureedge.net/pause-amd64:3.1 k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jun 13 03:19:56.046: INFO: &#xA;Logging kubelet events for node k8s-agentpool1-31296431-0&#xA;Jun 13 03:19:56.051: INFO: &#xA;Logging pods the kubelet thinks is on node k8s-agentpool1-31296431-0&#xA;Jun 13 03:19:56.062: INFO: heapster-6f7b9bdf-52ntm started at 2018-06-13 02:57:42 +0000 UTC (0+2 container statuses recorded)&#xA;Jun 13 03:19:56.062: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;Jun 13 03:19:56.062: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;Jun 13 03:19:56.062: INFO: downwardapi-volume-8f51369e-6eb8-11e8-a5be-000d3a184d46 started at 2018-06-13 03:19:19 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.062: INFO: &#x9;Container client-container ready: false, restart count 0&#xA;Jun 13 03:19:56.062: INFO: frontend-77c5d665c7-ls5tt started at 2018-06-13 03:19:09 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.062: INFO: &#x9;Container php-redis ready: true, restart count 0&#xA;Jun 13 03:19:56.062: INFO: downwardapi-volume-8afd8ffc-6eb8-11e8-a995-000d3a184d46 started at 2018-06-13 03:19:12 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.062: INFO: &#x9;Container client-container ready: false, restart count 0&#xA;Jun 13 03:19:56.062: INFO: kube-dns-v20-59b4f7dc55-phwv9 started at 2018-06-13 02:57:41 +0000 UTC (0+3 container statuses recorded)&#xA;Jun 13 03:19:56.062: INFO: &#x9;Container dnsmasq ready: true, restart count 0&#xA;Jun 13 03:19:56.062: INFO: &#x9;Container healthz ready: true, restart count 0&#xA;Jun 13 03:19:56.062: INFO: &#x9;Container kubedns ready: true, restart count 0&#xA;Jun 13 03:19:56.062: INFO: pod-configmaps-8bd86822-6eb8-11e8-baab-000d3a184d46 started at 2018-06-13 03:19:19 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.062: INFO: &#x9;Container configmap-volume-test ready: false, restart count 0&#xA;Jun 13 03:19:56.062: INFO: liveness-http started at 2018-06-13 03:19:19 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.062: INFO: &#x9;Container liveness ready: false, restart count 0&#xA;Jun 13 03:19:56.062: INFO: daemon-set-rgv4s started at 2018-06-13 03:19:37 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.062: INFO: &#x9;Container app ready: false, restart count 0&#xA;Jun 13 03:19:56.062: INFO: pod-configmaps-9bcf0289-6eb8-11e8-a315-000d3a184d46 started at 2018-06-13 03:19:40 +0000 UTC (0+3 container statuses recorded)&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container createcm-volume-test ready: false, restart count 0&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container delcm-volume-test ready: false, restart count 0&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container updcm-volume-test ready: false, restart count 0&#xA;Jun 13 03:19:56.063: INFO: metrics-server-5c74cf6d4f-h5mgt started at 2018-06-13 02:57:41 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;Jun 13 03:19:56.063: INFO: kube-dns-v20-59b4f7dc55-67fbv started at 2018-06-13 02:57:41 +0000 UTC (0+3 container statuses recorded)&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container dnsmasq ready: true, restart count 0&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container healthz ready: true, restart count 0&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container kubedns ready: true, restart count 0&#xA;Jun 13 03:19:56.063: INFO: kubernetes-dashboard-65c8bbc84b-tf6lc started at 2018-06-13 02:57:42 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;Jun 13 03:19:56.063: INFO: tiller-deploy-d85ccb55c-pk2j4 started at 2018-06-13 02:57:42 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container tiller ready: true, restart count 0&#xA;Jun 13 03:19:56.063: INFO: frontend-77c5d665c7-9hgkb started at 2018-06-13 03:19:09 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container php-redis ready: true, restart count 0&#xA;Jun 13 03:19:56.063: INFO: redis-master-6c74995565-8zl9q started at 2018-06-13 03:19:09 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container master ready: true, restart count 0&#xA;Jun 13 03:19:56.063: INFO: redis-slave-597bcc5997-7xp5n started at 2018-06-13 03:19:09 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container slave ready: true, restart count 0&#xA;Jun 13 03:19:56.063: INFO: kube-proxy-89kmg started at 2018-06-13 02:57:26 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container kube-proxy ready: true, restart count 0&#xA;Jun 13 03:19:56.063: INFO: pod-configmaps-8c9965ff-6eb8-11e8-b22e-000d3a184d46 started at 2018-06-13 03:19:19 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container configmap-volume-test ready: false, restart count 0&#xA;Jun 13 03:19:56.063: INFO: netserver-0 started at 2018-06-13 03:19:09 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.063: INFO: &#x9;Container webserver ready: true, restart count 0&#xA;Jun 13 03:19:56.218: INFO: &#xA;Latency metrics for node k8s-agentpool1-31296431-0&#xA;Jun 13 03:19:56.218: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:31.616884s}&#xA;Jun 13 03:19:56.218: INFO: {Operation:pull_image Method:docker_operations_latency_microseconds Quantile:0.99 Latency:13.94397s}&#xA;Jun 13 03:19:56.218: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.5 Latency:11.231301s}&#xA;Jun 13 03:19:56.218: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.9 Latency:11.231301s}&#xA;Jun 13 03:19:56.218: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.99 Latency:11.231301s}&#xA;Jun 13 03:19:56.218: INFO: &#xA;Logging node info for node k8s-agentpool1-31296431-1&#xA;Jun 13 03:19:56.221: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:k8s-agentpool1-31296431-1,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/k8s-agentpool1-31296431-1,UID:7879044f-6eb5-11e8-9654-000d3a134c0e,ResourceVersion:11031,Generation:0,CreationTimestamp:2018-06-13 02:57:12 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{agentpool: agentpool1,beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: Standard_F2,beta.kubernetes.io/os: linux,failure-domain.beta.kubernetes.io/region: eastus,failure-domain.beta.kubernetes.io/zone: 1,kubernetes.azure.com/cluster: t-xinhli-new3,kubernetes.io/hostname: k8s-agentpool1-31296431-1,kubernetes.io/role: agent,storageprofile: managed,storagetier: Standard_LRS,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:10.244.2.0/24,DoNotUse_ExternalID:,ProviderID:azure:///subscriptions/c4528d9e-c99a-48bb-b12d-fde2176a43b8/resourceGroups/t-xinhli-new3/providers/Microsoft.Compute/virtualMachines/k8s-agentpool1-31296431-1,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{31158935552 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4114132992 0} {&lt;nil&gt;} 4017708Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{28043041951 0} {&lt;nil&gt;} 28043041951 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4009275392 0} {&lt;nil&gt;} 3915308Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2018-06-13 02:58:57 +0000 UTC 2018-06-13 02:58:57 +0000 UTC RouteCreated RouteController created a route} {OutOfDisk False 2018-06-13 03:19:55 +0000 UTC 2018-06-13 02:57:12 +0000 UTC KubeletHasSufficientDisk kubelet has sufficient disk space available} {MemoryPressure False 2018-06-13 03:19:55 +0000 UTC 2018-06-13 02:57:12 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2018-06-13 03:19:55 +0000 UTC 2018-06-13 02:57:12 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2018-06-13 03:19:55 +0000 UTC 2018-06-13 02:57:12 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2018-06-13 03:19:55 +0000 UTC 2018-06-13 02:57:42 +0000 UTC KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.240.0.4} {Hostname k8s-agentpool1-31296431-1}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:61830f4d09b34235983a7fcd5f6f65d7,SystemUUID:08BFB559-3D6E-1842-B07C-D04E1987CF9D,BootID:0a8fe774-fb14-491c-9538-7f647358d372,KernelVersion:4.13.0-1018-azure,OSImage:Ubuntu 16.04.4 LTS,ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.12.0-alpha.0,KubeProxyVersion:v1.12.0-alpha.0,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcrio.azureedge.net/google_containers/hyperkube-amd64@sha256:b92fc22f40fa4bf5d72ddd6e82ffd5d5a98ac9201eeb086843899f3a9fbf3eee gcrio.azureedge.net/google_containers/hyperkube-amd64:v1.12.0-alpha.0] 619581352} {[k8s.gcr.io/nginx-slim-amd64@sha256:6654db6d4028756062edac466454ee5c9cf9b20ef79e35a81e3c840031eb1e2b k8s.gcr.io/nginx-slim-amd64:0.20] 103591055} {[k8s.gcr.io/nginx-slim-amd64@sha256:8ca6a9ecef3b2ef02f6e0c3d449235d9c53d532f420cc0a29a6a133aa88df256 k8s.gcr.io/nginx-slim-amd64:0.21] 95339966} {[gcr.io/kubernetes-e2e-test-images/nettest-amd64@sha256:ff598458029b42e23b823a3a690c07e1f6921627f3fc49d007033494eca13141 gcr.io/kubernetes-e2e-test-images/nettest-amd64:1.0] 30381916} {[gcr.io/kubernetes-e2e-test-images/hostexec-amd64@sha256:bdaecec5adfa7c79e9525c0992fdab36c2d68066f5e91eff0d1d9e8d73c654ea gcr.io/kubernetes-e2e-test-images/hostexec-amd64:1.1] 8407119} {[gcr.io/kubernetes-e2e-test-images/netexec-amd64@sha256:2edfad424a541b9e024f26368d3a5b7dcc1d7cd27a4ee8c1d8c3f81d9209ab2e gcr.io/kubernetes-e2e-test-images/netexec-amd64:1.0] 6227659} {[gcr.io/kubernetes-e2e-test-images/redis-amd64@sha256:3e01bcaf67cb9b5c9fa7f57ba92539c8962d59c9647b91e9ec5047a89e2bc49a gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0] 5850779} {[gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64@sha256:2dd4032e98a0450d95a0ac71a5e465f542a900812d8c41bc6ca635aed1a5fc91 gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0] 5470001} {[gcr.io/kubernetes-e2e-test-images/nautilus-amd64@sha256:a4e859e40750d0e1a94470445bea3bc0ba4edc7363863a0be7a3714336040daa gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0] 4415165} {[gcr.io/kubernetes-e2e-test-images/kitten-amd64@sha256:36e05bc3dfe1cd0f2d2d119f47fbb6e1c75037b291073012d57fbf28af3de3d7 gcr.io/kubernetes-e2e-test-images/kitten-amd64:1.0] 4408701} {[gcr.io/kubernetes-e2e-test-images/porter-amd64@sha256:b5923cab026ae1a96325823e10254c9beda240126355a20ca64b59445ec631bb gcr.io/kubernetes-e2e-test-images/porter-amd64:1.0] 4400998} {[gcr.io/kubernetes-e2e-test-images/test-webserver-amd64@sha256:cd237408ae94e22c4f5c6d7c6f56708341db6c428180fe1fe011c17bf9d03c50 gcr.io/kubernetes-e2e-test-images/test-webserver-amd64:1.0] 4393904} {[gcr.io/kubernetes-e2e-test-images/liveness-amd64@sha256:5f89fb4e972426ae8d1d60dcf041f631f4d79922f50cc7ecbac2a8d1ed66edae gcr.io/kubernetes-e2e-test-images/liveness-amd64:1.0] 4279645} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester-amd64@sha256:ed08ffff86b0a2016fa650ff76ea4b85fc3dfcab30ee4b7bd47d514ab765da89 gcr.io/kubernetes-e2e-test-images/entrypoint-tester-amd64:1.0] 2276578} {[gcr.io/kubernetes-e2e-test-images/mounttest-user-amd64@sha256:dda6519a95c934b46731a6b1492fed1b48ccc6d4aed4b754a46d7de8063a3e2b gcr.io/kubernetes-e2e-test-images/mounttest-user-amd64:1.0] 1450451} {[gcr.io/kubernetes-e2e-test-images/mounttest-amd64@sha256:dc4e2dcfbde16249c4662de673295d00778577bc2e2ca7013a1b85d4f47398ca gcr.io/kubernetes-e2e-test-images/mounttest-amd64:1.0] 1450451} {[busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47 busybox:latest] 1146369} {[k8s-gcrio.azureedge.net/pause-amd64@sha256:59eec8837a4d942cc19a52b8c09ea75121acc38114a2c68b98983ce9356b8610 k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s-gcrio.azureedge.net/pause-amd64:3.1 k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jun 13 03:19:56.221: INFO: &#xA;Logging kubelet events for node k8s-agentpool1-31296431-1&#xA;Jun 13 03:19:56.225: INFO: &#xA;Logging pods the kubelet thinks is on node k8s-agentpool1-31296431-1&#xA;Jun 13 03:19:56.240: INFO: pod-projected-secrets-8070ab60-6eb8-11e8-8891-000d3a184d46 started at 2018-06-13 03:18:54 +0000 UTC (0+3 container statuses recorded)&#xA;Jun 13 03:19:56.240: INFO: &#x9;Container creates-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:56.240: INFO: &#x9;Container dels-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:56.240: INFO: &#x9;Container upds-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:56.240: INFO: liveness-http started at 2018-06-13 03:16:08 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.240: INFO: &#x9;Container liveness ready: true, restart count 0&#xA;Jun 13 03:19:56.240: INFO: daemon-set-v6w87 started at 2018-06-13 03:19:39 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.240: INFO: &#x9;Container app ready: true, restart count 0&#xA;Jun 13 03:19:56.240: INFO: netserver-1 started at 2018-06-13 03:19:09 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.240: INFO: &#x9;Container webserver ready: false, restart count 0&#xA;Jun 13 03:19:56.240: INFO: frontend-77c5d665c7-5fkvl started at 2018-06-13 03:19:09 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.240: INFO: &#x9;Container php-redis ready: true, restart count 0&#xA;Jun 13 03:19:56.240: INFO: pod-secrets-727f66e9-6eb8-11e8-addd-000d3a184d46 started at 2018-06-13 03:18:30 +0000 UTC (0+3 container statuses recorded)&#xA;Jun 13 03:19:56.240: INFO: &#x9;Container creates-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:56.240: INFO: &#x9;Container dels-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:56.240: INFO: &#x9;Container upds-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:56.240: INFO: redis-slave-597bcc5997-k94w4 started at 2018-06-13 03:19:09 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.240: INFO: &#x9;Container slave ready: true, restart count 0&#xA;Jun 13 03:19:56.240: INFO: kube-proxy-fr77g started at 2018-06-13 02:57:26 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.240: INFO: &#x9;Container kube-proxy ready: true, restart count 0&#xA;Jun 13 03:19:56.269: INFO: &#xA;Latency metrics for node k8s-agentpool1-31296431-1&#xA;Jun 13 03:19:56.269: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.9 Latency:2m3.040809s}&#xA;Jun 13 03:19:56.269: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.99 Latency:2m3.040809s}&#xA;Jun 13 03:19:56.269: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:27.953243s}&#xA;Jun 13 03:19:56.269: INFO: &#xA;Logging node info for node k8s-master-31296431-0&#xA;Jun 13 03:19:56.272: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:k8s-master-31296431-0,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/k8s-master-31296431-0,UID:74e2fb02-6eb5-11e8-9654-000d3a134c0e,ResourceVersion:10959,Generation:0,CreationTimestamp:2018-06-13 02:57:06 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: Standard_F2,beta.kubernetes.io/os: linux,failure-domain.beta.kubernetes.io/region: eastus,failure-domain.beta.kubernetes.io/zone: 0,kubernetes.azure.com/cluster: t-xinhli-new3,kubernetes.io/hostname: k8s-master-31296431-0,kubernetes.io/role: master,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:10.244.0.0/24,DoNotUse_ExternalID:,ProviderID:azure:///subscriptions/c4528d9e-c99a-48bb-b12d-fde2176a43b8/resourceGroups/t-xinhli-new3/providers/Microsoft.Compute/virtualMachines/k8s-master-31296431-0,Unschedulable:false,Taints:[{node-role.kubernetes.io/master true NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{31158935552 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4114132992 0} {&lt;nil&gt;} 4017708Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{28043041951 0} {&lt;nil&gt;} 28043041951 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4009275392 0} {&lt;nil&gt;} 3915308Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2018-06-13 02:59:10 +0000 UTC 2018-06-13 02:59:10 +0000 UTC RouteCreated RouteController created a route} {OutOfDisk False 2018-06-13 03:19:48 +0000 UTC 2018-06-13 02:56:57 +0000 UTC KubeletHasSufficientDisk kubelet has sufficient disk space available} {MemoryPressure False 2018-06-13 03:19:48 +0000 UTC 2018-06-13 02:56:57 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2018-06-13 03:19:48 +0000 UTC 2018-06-13 02:56:57 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2018-06-13 03:19:48 +0000 UTC 2018-06-13 02:56:57 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2018-06-13 03:19:48 +0000 UTC 2018-06-13 02:57:36 +0000 UTC KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.240.255.5} {Hostname k8s-master-31296431-0}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:55cfce2841774bb4acf59a2b85bb2c17,SystemUUID:5E9AA3FE-6AB1-FF4E-8B54-9A53FE5E1C1C,BootID:ab62f162-195c-4fcc-87c7-9dd1fbb45e4b,KernelVersion:4.13.0-1018-azure,OSImage:Ubuntu 16.04.4 LTS,ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.12.0-alpha.0,KubeProxyVersion:v1.12.0-alpha.0,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcrio.azureedge.net/google_containers/hyperkube-amd64@sha256:b92fc22f40fa4bf5d72ddd6e82ffd5d5a98ac9201eeb086843899f3a9fbf3eee gcrio.azureedge.net/google_containers/hyperkube-amd64:v1.12.0-alpha.0] 619581352} {[fseldow/azure-cloud-controller-manager@sha256:8e41588ea607b1104fd0d785a17f9694410710c09419a455b91da8053985d7df fseldow/azure-cloud-controller-manager:e065e27] 168173406} {[k8s-gcrio.azureedge.net/kube-addon-manager-amd64@sha256:3519273916ba45cfc9b318448d4629819cb5fbccbb0822cce054dd8c1f68cb60 k8s-gcrio.azureedge.net/kube-addon-manager-amd64:v8.6] 78384272} {[k8s-gcrio.azureedge.net/pause-amd64@sha256:59eec8837a4d942cc19a52b8c09ea75121acc38114a2c68b98983ce9356b8610 k8s-gcrio.azureedge.net/pause-amd64:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jun 13 03:19:56.273: INFO: &#xA;Logging kubelet events for node k8s-master-31296431-0&#xA;Jun 13 03:19:56.277: INFO: &#xA;Logging pods the kubelet thinks is on node k8s-master-31296431-0&#xA;Jun 13 03:19:56.281: INFO: cloud-controller-manager-k8s-master-31296431-0 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jun 13 03:19:56.281: INFO: kube-addon-manager-k8s-master-31296431-0 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jun 13 03:19:56.281: INFO: kube-apiserver-k8s-master-31296431-0 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jun 13 03:19:56.281: INFO: kube-controller-manager-k8s-master-31296431-0 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jun 13 03:19:56.281: INFO: kube-proxy-5976x started at 2018-06-13 02:57:26 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:56.281: INFO: &#x9;Container kube-proxy ready: true, restart count 0&#xA;Jun 13 03:19:56.281: INFO: kube-scheduler-k8s-master-31296431-0 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jun 13 03:19:56.347: INFO: &#xA;Latency metrics for node k8s-master-31296431-0&#xA;�[1mSTEP�[0m: Dumping a list of prepulled images on each node to file /home/t-xinhli/new-provider/cloud-provider-azure/t-xinhli-new3/report/image-puller.txt&#xA;Jun 13 03:19:56.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;e2e-tests-sched-pred-7tktn&#34; for this suite.&#xA;Jun 13 03:20:02.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jun 13 03:20:02.554: INFO: namespace: e2e-tests-sched-pred-7tktn, resource: bindings, ignored listing per whitelist&#xA;Jun 13 03:20:02.577: INFO: namespace e2e-tests-sched-pred-7tktn deletion completed in 6.222589817s&#xA;[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /home/t-xinhli/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70&#xA;</system-out>
      </testcase>
  </testsuite>