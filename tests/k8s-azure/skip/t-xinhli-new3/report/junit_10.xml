<?xml version="1.0" encoding="UTF-8"?>
  <testsuite tests="10" failures="1" time="304.932470754">
      <testcase name="[sig-storage] Mounted volume expand[Slow] Should verify mounted devices can be resized" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should be able to create an internal type load balancer [Slow] [DisabledForLargeClusters]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cluster-lifecycle] Reboot [Disruptive] [Feature:Reboot] each node by dropping all outbound packets for a while and ensure they function afterwards" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] PodSecurityPolicy should allow pods under the privileged extensions.PodSecurityPolicy" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath [Volume type: hostPathSymlink] should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GenericPersistentVolume[Disruptive] When kubelet restarts Should test that a volume mounted to a pod that is deleted while the kubelet is down unmounts when the kubelet returns." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Storage Policy Based Volume Provisioning [Feature:vsphere] verify VSAN storage capability with invalid capability name objectSpaceReserve is not honored for dynamically provisioned pvc using storageclass" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: Custom Metrics from Stackdriver) should scale down with Custom Metric of type Object from Stackdriver [Feature:CustomMetricsAutoscaling]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]" classname="Kubernetes e2e suite" time="42.086998814"></testcase>
      <testcase name="[sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : configmap" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Regional PD RegionalPD should failover to a different zone when all nodes in one zone become unreachable [Slow] [Disruptive]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] [sig-node] Security Context [Feature:SecurityContext] should support seccomp alpha unconfined annotation on the pod [Feature:Seccomp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] Daemon set [Serial] should not update pod when spec was updated and update strategy is OnDelete" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl expose should create services for rc  [Conformance]" classname="Kubernetes e2e suite" time="35.331439308"></testcase>
      <testcase name="[sig-network] Services should be able to update NodePorts with two same port numbers but different protocols" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="14.557660028"></testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should increase cluster size if pods are pending due to pod anti-affinity [Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Probing container should be restarted with a docker exec liveness probe with timeout " classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath [Volume type: emptyDir] should unmount if pod is force deleted while kubelet is down [Disruptive][Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and write from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should not deadlock when a pod&#39;s predecessor fails" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule pods each with a PD, delete pod and verify detach [Slow] for read-only PD with pod delete grace period of &#34;default (30s)&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Loadbalancing: L7 GCE [Slow] [Feature:kubemci] should create ingress with backend HTTPS" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="18.339979158"></testcase>
      <testcase name="[sig-storage] Subpath [Volume type: hostPathSymlink] should support readOnly file specified in the volumeMount" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  Local volume provisioner [Serial] should not create local persistent volume for filesystem volume that was not bind mounted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] [Serial] Volume metrics PVController should create none metrics for pvc controller before creating any PV or PVC" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Networking Granular Checks: Services [Slow] should function for node-Service: udp" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] ReplicationController should release no longer matching pods" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using { ReplicationController} with 0 secrets, 0 configmaps and 2 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] ResourceQuota should verify ResourceQuota with terminating scopes." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] EquivalenceCache [Serial] validates pod affinity works properly when new replica pod is scheduled" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-auth] [Feature:NodeAuthorizer] Getting an existing configmap should exit with the Forbidden error" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath [Volume type: hostPath] should support file as subpath" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="26.969794494"></testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that taints-tolerations is respected if not matching" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-instrumentation] [Feature:PrometheusMonitoring] Prometheus should successfully scrape all targets" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath [Volume type: hostPath] should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] shouldn&#39;t scale up if cores limit too low, should scale up after limit is changed [Feature:ClusterSizeAutoscalingScaleWithNAP]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should scale up GPU pool from 0 [GpuType:nvidia-tesla-p100] [Feature:ClusterSizeAutoscalingGpu]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: block] [Feature:BlockVolume] Set fsGroup for local volume should set fsGroup for one pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-apps] CronJob should schedule multiple jobs concurrently" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="19.147704246"></testcase>
      <testcase name="[sig-scheduling] ResourceQuota should create a ResourceQuota and capture the life of a service." classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath [Volume type: gluster] should support existing directory" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] Should be able to scale a node group up from 0[Feature:ClusterSizeAutoscalingScaleUp]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] NetworkPolicy NetworkPolicy between server and client should allow egress access on one named port [Feature:NetworkPolicy]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-autoscaling] Cluster size autoscaling [Slow] should add new node and new node pool on too big pod, scale down to 1 and scale down to 0 [Feature:ClusterSizeAutoscalingScaleWithNAP]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should mutate configmap" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath [Volume type: gluster] should fail for new directories when readOnly specified in the volumeSource" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] Multi-AZ Clusters should spread the pods of a service across zones" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scalability] Density [Feature:ManualPerformance] should allow starting 30 pods per node using {extensions Deployment} with 2 secrets, 0 configmaps and 0 daemons" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set fsGroup for one pod" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] GCP Volumes NFSv4 should be mountable for NFSv4" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath [Volume type: gcePDPartitioned] should fail if non-existent subpath is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement test back to back pod creation and deletion with different volume sources on the same worker node" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[k8s.io] GKE local SSD [Feature:GKELocalSSD] should write and read from node local SSD [Feature:GKELocalSSD]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-api-machinery] Downward API should provide container&#39;s limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="16.468495664"></testcase>
      <testcase name="[sig-storage] Subpath [Volume type: gcePDPVC] should fail if subpath directory is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Volume Placement should create and delete pod with multiple volumes from same datastore" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Projected should be consumable from pods in volume with mappings as non-root with FSGroup [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Pod Disks schedule pods each with a PD, delete pod and verify detach [Slow] for RW PD with pod delete grace period of &#34;immediate (0s)&#34;" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]" classname="Kubernetes e2e suite" time="66.59770896">
          <failure type="Failure">/home/t-xinhli/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79&#xA;Expected error:&#xA;    &lt;*errors.errorString | 0xc42091f4e0&gt;: {&#xA;        s: &#34;Namespace e2e-tests-configmap-9kznn is active&#34;,&#xA;    }&#xA;    Namespace e2e-tests-configmap-9kznn is active&#xA;not to have occurred&#xA;/home/t-xinhli/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:88</failure>
          <system-out>[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /home/t-xinhli/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141&#xA;�[1mSTEP�[0m: Creating a kubernetes client&#xA;Jun 13 03:18:12.778: INFO: &gt;&gt;&gt; kubeConfig: /home/t-xinhli/new-provider/cloud-provider-azure/t-xinhli-new3/kubeconfig&#xA;�[1mSTEP�[0m: Building a namespace api object&#xA;�[1mSTEP�[0m: Waiting for a default service account to be provisioned in namespace&#xA;[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /home/t-xinhli/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79&#xA;Jun 13 03:18:12.924: INFO: Waiting up to 1m0s for all nodes to be ready&#xA;Jun 13 03:19:12.958: INFO: Waiting for terminating namespaces to be deleted...&#xA;Jun 13 03:19:12.961: INFO: Unexpected error occurred: Namespace e2e-tests-configmap-9kznn is active&#xA;[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /home/t-xinhli/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142&#xA;�[1mSTEP�[0m: Collecting events from namespace &#34;e2e-tests-sched-pred-28bkn&#34;.&#xA;�[1mSTEP�[0m: Found 0 events.&#xA;Jun 13 03:19:12.976: INFO: POD                                                            NODE                       PHASE    GRACE  CONDITIONS&#xA;Jun 13 03:19:12.976: INFO: pod-configmaps-8adf423c-6eb8-11e8-a8e2-000d3a184d46            k8s-agentpool1-31296431-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:11 +0000 UTC  }]&#xA;Jun 13 03:19:12.976: INFO: liveness-http                                                  k8s-agentpool1-31296431-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:16:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:16:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:16:08 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: liveness-exec                                                  k8s-agentpool1-31296431-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:15:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:15:44 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:15:41 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: frontend-77c5d665c7-5fkvl                                      k8s-agentpool1-31296431-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:08 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: frontend-77c5d665c7-9hgkb                                      k8s-agentpool1-31296431-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC ContainersNotReady containers with unready status: [php-redis]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:08 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: frontend-77c5d665c7-ls5tt                                      k8s-agentpool1-31296431-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC ContainersNotReady containers with unready status: [php-redis]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:08 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: redis-master-6c74995565-8zl9q                                  k8s-agentpool1-31296431-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC ContainersNotReady containers with unready status: [master]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: redis-slave-597bcc5997-7xp5n                                   k8s-agentpool1-31296431-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC ContainersNotReady containers with unready status: [slave]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: redis-slave-597bcc5997-k94w4                                   k8s-agentpool1-31296431-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: update-demo-kitten-bvwlm                                       k8s-agentpool1-31296431-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:02 +0000 UTC ContainersNotReady containers with unready status: [update-demo]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:38 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: netserver-0                                                    k8s-agentpool1-31296431-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: netserver-1                                                    k8s-agentpool1-31296431-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:09 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: pod-projected-secrets-8070ab60-6eb8-11e8-8891-000d3a184d46     k8s-agentpool1-31296431-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:54 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: downwardapi-volume-8afd8ffc-6eb8-11e8-a995-000d3a184d46        k8s-agentpool1-31296431-0  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:12 +0000 UTC ContainersNotReady containers with unready status: [client-container]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:11 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: labelsupdate854888ee-6eb8-11e8-8768-000d3a184d46               k8s-agentpool1-31296431-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:02 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: pod-projected-configmaps-64e93643-6eb8-11e8-b5c9-000d3a184d46  k8s-agentpool1-31296431-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:08 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: pod-secrets-727f66e9-6eb8-11e8-addd-000d3a184d46               k8s-agentpool1-31296431-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:30 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: ss-0                                                           k8s-agentpool1-31296431-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:18:44 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: ss-1                                                           k8s-agentpool1-31296431-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:05 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: ss-2                                                           k8s-agentpool1-31296431-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 03:19:05 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: cloud-controller-manager-k8s-master-31296431-0                 k8s-master-31296431-0      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: heapster-6f7b9bdf-52ntm                                        k8s-agentpool1-31296431-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:58:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:42 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: kube-addon-manager-k8s-master-31296431-0                       k8s-master-31296431-0      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: kube-apiserver-k8s-master-31296431-0                           k8s-master-31296431-0      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: kube-controller-manager-k8s-master-31296431-0                  k8s-master-31296431-0      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: kube-dns-v20-59b4f7dc55-67fbv                                  k8s-agentpool1-31296431-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:58:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:41 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: kube-dns-v20-59b4f7dc55-phwv9                                  k8s-agentpool1-31296431-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:58:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:41 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: kube-proxy-5976x                                               k8s-master-31296431-0      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:29 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:26 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: kube-proxy-89kmg                                               k8s-agentpool1-31296431-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:26 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: kube-proxy-fr77g                                               k8s-agentpool1-31296431-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:26 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: kube-scheduler-k8s-master-31296431-0                           k8s-master-31296431-0      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:56:50 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: kubernetes-dashboard-65c8bbc84b-tf6lc                          k8s-agentpool1-31296431-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:53 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:42 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: metrics-server-5c74cf6d4f-h5mgt                                k8s-agentpool1-31296431-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:40 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: tiller-deploy-d85ccb55c-pk2j4                                  k8s-agentpool1-31296431-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:58:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2018-06-13 02:57:42 +0000 UTC  }]&#xA;Jun 13 03:19:12.977: INFO: &#xA;Jun 13 03:19:12.980: INFO: &#xA;Logging node info for node k8s-agentpool1-31296431-0&#xA;Jun 13 03:19:12.983: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:k8s-agentpool1-31296431-0,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/k8s-agentpool1-31296431-0,UID:74eaa19b-6eb5-11e8-9654-000d3a134c0e,ResourceVersion:10358,Generation:0,CreationTimestamp:2018-06-13 02:57:06 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{agentpool: agentpool1,beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: Standard_F2,beta.kubernetes.io/os: linux,failure-domain.beta.kubernetes.io/region: eastus,failure-domain.beta.kubernetes.io/zone: 0,kubernetes.azure.com/cluster: t-xinhli-new3,kubernetes.io/hostname: k8s-agentpool1-31296431-0,kubernetes.io/role: agent,storageprofile: managed,storagetier: Standard_LRS,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:10.244.1.0/24,DoNotUse_ExternalID:,ProviderID:azure:///subscriptions/c4528d9e-c99a-48bb-b12d-fde2176a43b8/resourceGroups/t-xinhli-new3/providers/Microsoft.Compute/virtualMachines/k8s-agentpool1-31296431-0,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{31158935552 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4114132992 0} {&lt;nil&gt;} 4017708Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{28043041951 0} {&lt;nil&gt;} 28043041951 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4009275392 0} {&lt;nil&gt;} 3915308Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2018-06-13 02:58:57 +0000 UTC 2018-06-13 02:58:57 +0000 UTC RouteCreated RouteController created a route} {OutOfDisk False 2018-06-13 03:19:08 +0000 UTC 2018-06-13 02:57:05 +0000 UTC KubeletHasSufficientDisk kubelet has sufficient disk space available} {MemoryPressure False 2018-06-13 03:19:08 +0000 UTC 2018-06-13 02:57:05 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2018-06-13 03:19:08 +0000 UTC 2018-06-13 02:57:05 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2018-06-13 03:19:08 +0000 UTC 2018-06-13 02:57:05 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2018-06-13 03:19:08 +0000 UTC 2018-06-13 02:57:36 +0000 UTC KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.240.0.5} {Hostname k8s-agentpool1-31296431-0}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:79fe54da919d45ff9e81f9bf6ebd36fb,SystemUUID:24B4B64B-ECDB-7144-A474-9331C73E23DF,BootID:3b74ef78-7064-4109-a243-e752fa302910,KernelVersion:4.13.0-1018-azure,OSImage:Ubuntu 16.04.4 LTS,ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.12.0-alpha.0,KubeProxyVersion:v1.12.0-alpha.0,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcrio.azureedge.net/google_containers/hyperkube-amd64@sha256:b92fc22f40fa4bf5d72ddd6e82ffd5d5a98ac9201eeb086843899f3a9fbf3eee gcrio.azureedge.net/google_containers/hyperkube-amd64:v1.12.0-alpha.0] 619581352} {[gcr.io/kubernetes-e2e-test-images/jessie-dnsutils-amd64@sha256:b7fb726f862504ca6afcc512abf30456cbf4fbd6b4589af46d2c1d27cb89acf0 gcr.io/kubernetes-e2e-test-images/jessie-dnsutils-amd64:1.0] 189192551} {[k8s.gcr.io/nginx-slim-amd64@sha256:6654db6d4028756062edac466454ee5c9cf9b20ef79e35a81e3c840031eb1e2b k8s.gcr.io/nginx-slim-amd64:0.20] 103591055} {[k8s-gcrio.azureedge.net/kubernetes-dashboard-amd64@sha256:dc4026c1b595435ef5527ca598e1e9c4343076926d7d62b365c44831395adbd0 k8s-gcrio.azureedge.net/kubernetes-dashboard-amd64:v1.8.3] 102319441} {[k8s.gcr.io/nginx-slim-amd64@sha256:8ca6a9ecef3b2ef02f6e0c3d449235d9c53d532f420cc0a29a6a133aa88df256 k8s.gcr.io/nginx-slim-amd64:0.21] 95339966} {[k8s-gcrio.azureedge.net/heapster-amd64@sha256:d4d10455d921802bdb004e7edfe423a2b2f88911319b48abf47e0af909f27f15 k8s-gcrio.azureedge.net/heapster-amd64:v1.5.1] 75318380} {[gcrio.azureedge.net/kubernetes-helm/tiller@sha256:394fb7d5f2fbaca54f6a0dec387cef926f6ae359786c89f7da67db173b97a322 gcrio.azureedge.net/kubernetes-helm/tiller:v2.8.1] 71509364} {[k8s-gcrio.azureedge.net/k8s-dns-kube-dns-amd64@sha256:6d8e0da4fb46e9ea2034a3f4cab0e095618a2ead78720c12e791342738e5f85d k8s-gcrio.azureedge.net/k8s-dns-kube-dns-amd64:1.14.8] 50456751} {[k8s-gcrio.azureedge.net/metrics-server-amd64@sha256:49a9f12f7067d11f42c803dbe61ed2c1299959ad85cb315b25ff7eef8e6b8892 k8s-gcrio.azureedge.net/metrics-server-amd64:v0.2.1] 42541759} {[k8s-gcrio.azureedge.net/k8s-dns-dnsmasq-nanny-amd64@sha256:93c827f018cf3322f1ff2aa80324a0306048b0a69bc274e423071fb0d2d29d8b k8s-gcrio.azureedge.net/k8s-dns-dnsmasq-nanny-amd64:1.14.8] 40951779} {[k8s-gcrio.azureedge.net/addon-resizer@sha256:507aa9845ecce1fdde4d61f530c802f4dc2974c700ce0db7730866e442db958d k8s-gcrio.azureedge.net/addon-resizer:1.8.1] 32968591} {[gcr.io/kubernetes-e2e-test-images/dnsutils-amd64@sha256:9e1873dbe19894688dc3d129994c952891d43794df2480217f899bf92732d35f gcr.io/kubernetes-e2e-test-images/dnsutils-amd64:1.0] 8871636} {[k8s-gcrio.azureedge.net/exechealthz-amd64@sha256:503e158c3f65ed7399f54010571c7c977ade7fe59010695f48d9650d83488c0a k8s-gcrio.azureedge.net/exechealthz-amd64:1.2] 8374840} {[gcr.io/kubernetes-e2e-test-images/netexec-amd64@sha256:2edfad424a541b9e024f26368d3a5b7dcc1d7cd27a4ee8c1d8c3f81d9209ab2e gcr.io/kubernetes-e2e-test-images/netexec-amd64:1.0] 6227659} {[gcr.io/kubernetes-e2e-test-images/redis-amd64@sha256:3e01bcaf67cb9b5c9fa7f57ba92539c8962d59c9647b91e9ec5047a89e2bc49a gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0] 5850779} {[gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64@sha256:2dd4032e98a0450d95a0ac71a5e465f542a900812d8c41bc6ca635aed1a5fc91 gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0] 5470001} {[gcr.io/kubernetes-e2e-test-images/nautilus-amd64@sha256:a4e859e40750d0e1a94470445bea3bc0ba4edc7363863a0be7a3714336040daa gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0] 4415165} {[gcr.io/kubernetes-e2e-test-images/kitten-amd64@sha256:36e05bc3dfe1cd0f2d2d119f47fbb6e1c75037b291073012d57fbf28af3de3d7 gcr.io/kubernetes-e2e-test-images/kitten-amd64:1.0] 4408701} {[gcr.io/kubernetes-e2e-test-images/test-webserver-amd64@sha256:cd237408ae94e22c4f5c6d7c6f56708341db6c428180fe1fe011c17bf9d03c50 gcr.io/kubernetes-e2e-test-images/test-webserver-amd64:1.0] 4393904} {[gcr.io/kubernetes-e2e-test-images/mounttest-amd64@sha256:dc4e2dcfbde16249c4662de673295d00778577bc2e2ca7013a1b85d4f47398ca gcr.io/kubernetes-e2e-test-images/mounttest-amd64:1.0] 1450451} {[gcr.io/kubernetes-e2e-test-images/mounttest-user-amd64@sha256:dda6519a95c934b46731a6b1492fed1b48ccc6d4aed4b754a46d7de8063a3e2b gcr.io/kubernetes-e2e-test-images/mounttest-user-amd64:1.0] 1450451} {[busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47 busybox:latest] 1146369} {[k8s-gcrio.azureedge.net/pause-amd64@sha256:59eec8837a4d942cc19a52b8c09ea75121acc38114a2c68b98983ce9356b8610 k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s-gcrio.azureedge.net/pause-amd64:3.1 k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jun 13 03:19:12.983: INFO: &#xA;Logging kubelet events for node k8s-agentpool1-31296431-0&#xA;Jun 13 03:19:12.987: INFO: &#xA;Logging pods the kubelet thinks is on node k8s-agentpool1-31296431-0&#xA;Jun 13 03:19:13.003: INFO: tiller-deploy-d85ccb55c-pk2j4 started at 2018-06-13 02:57:42 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container tiller ready: true, restart count 0&#xA;Jun 13 03:19:13.003: INFO: ss-1 started at 2018-06-13 03:19:05 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container nginx ready: true, restart count 0&#xA;Jun 13 03:19:13.003: INFO: frontend-77c5d665c7-9hgkb started at 2018-06-13 03:19:09 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container php-redis ready: false, restart count 0&#xA;Jun 13 03:19:13.003: INFO: redis-master-6c74995565-8zl9q started at 2018-06-13 03:19:09 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container master ready: false, restart count 0&#xA;Jun 13 03:19:13.003: INFO: redis-slave-597bcc5997-7xp5n started at 2018-06-13 03:19:09 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container slave ready: false, restart count 0&#xA;Jun 13 03:19:13.003: INFO: kube-proxy-89kmg started at 2018-06-13 02:57:26 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container kube-proxy ready: true, restart count 0&#xA;Jun 13 03:19:13.003: INFO: kube-dns-v20-59b4f7dc55-67fbv started at 2018-06-13 02:57:41 +0000 UTC (0+3 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container dnsmasq ready: true, restart count 0&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container healthz ready: true, restart count 0&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container kubedns ready: true, restart count 0&#xA;Jun 13 03:19:13.003: INFO: kubernetes-dashboard-65c8bbc84b-tf6lc started at 2018-06-13 02:57:42 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container kubernetes-dashboard ready: true, restart count 0&#xA;Jun 13 03:19:13.003: INFO: netserver-0 started at 2018-06-13 03:19:09 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container webserver ready: false, restart count 0&#xA;Jun 13 03:19:13.003: INFO: frontend-77c5d665c7-ls5tt started at 2018-06-13 03:19:09 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container php-redis ready: false, restart count 0&#xA;Jun 13 03:19:13.003: INFO: downwardapi-volume-8afd8ffc-6eb8-11e8-a995-000d3a184d46 started at 2018-06-13 03:19:12 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container client-container ready: false, restart count 0&#xA;Jun 13 03:19:13.003: INFO: kube-dns-v20-59b4f7dc55-phwv9 started at 2018-06-13 02:57:41 +0000 UTC (0+3 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container dnsmasq ready: true, restart count 0&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container healthz ready: true, restart count 0&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container kubedns ready: true, restart count 0&#xA;Jun 13 03:19:13.003: INFO: heapster-6f7b9bdf-52ntm started at 2018-06-13 02:57:42 +0000 UTC (0+2 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container heapster ready: true, restart count 0&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container heapster-nanny ready: true, restart count 0&#xA;Jun 13 03:19:13.003: INFO: update-demo-kitten-bvwlm started at 2018-06-13 03:18:38 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container update-demo ready: false, restart count 0&#xA;Jun 13 03:19:13.003: INFO: metrics-server-5c74cf6d4f-h5mgt started at 2018-06-13 02:57:41 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.003: INFO: &#x9;Container metrics-server ready: true, restart count 0&#xA;Jun 13 03:19:13.040: INFO: &#xA;Latency metrics for node k8s-agentpool1-31296431-0&#xA;Jun 13 03:19:13.040: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:13.009428s}&#xA;Jun 13 03:19:13.040: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.5 Latency:11.231301s}&#xA;Jun 13 03:19:13.040: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.9 Latency:11.231301s}&#xA;Jun 13 03:19:13.040: INFO: {Operation:create Method:pod_worker_latency_microseconds Quantile:0.99 Latency:11.231301s}&#xA;Jun 13 03:19:13.040: INFO: &#xA;Logging node info for node k8s-agentpool1-31296431-1&#xA;Jun 13 03:19:13.043: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:k8s-agentpool1-31296431-1,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/k8s-agentpool1-31296431-1,UID:7879044f-6eb5-11e8-9654-000d3a134c0e,ResourceVersion:10263,Generation:0,CreationTimestamp:2018-06-13 02:57:12 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{agentpool: agentpool1,beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: Standard_F2,beta.kubernetes.io/os: linux,failure-domain.beta.kubernetes.io/region: eastus,failure-domain.beta.kubernetes.io/zone: 1,kubernetes.azure.com/cluster: t-xinhli-new3,kubernetes.io/hostname: k8s-agentpool1-31296431-1,kubernetes.io/role: agent,storageprofile: managed,storagetier: Standard_LRS,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:10.244.2.0/24,DoNotUse_ExternalID:,ProviderID:azure:///subscriptions/c4528d9e-c99a-48bb-b12d-fde2176a43b8/resourceGroups/t-xinhli-new3/providers/Microsoft.Compute/virtualMachines/k8s-agentpool1-31296431-1,Unschedulable:false,Taints:[],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{31158935552 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4114132992 0} {&lt;nil&gt;} 4017708Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{28043041951 0} {&lt;nil&gt;} 28043041951 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4009275392 0} {&lt;nil&gt;} 3915308Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2018-06-13 02:58:57 +0000 UTC 2018-06-13 02:58:57 +0000 UTC RouteCreated RouteController created a route} {OutOfDisk False 2018-06-13 03:19:04 +0000 UTC 2018-06-13 02:57:12 +0000 UTC KubeletHasSufficientDisk kubelet has sufficient disk space available} {MemoryPressure False 2018-06-13 03:19:04 +0000 UTC 2018-06-13 02:57:12 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2018-06-13 03:19:04 +0000 UTC 2018-06-13 02:57:12 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2018-06-13 03:19:04 +0000 UTC 2018-06-13 02:57:12 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2018-06-13 03:19:04 +0000 UTC 2018-06-13 02:57:42 +0000 UTC KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.240.0.4} {Hostname k8s-agentpool1-31296431-1}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:61830f4d09b34235983a7fcd5f6f65d7,SystemUUID:08BFB559-3D6E-1842-B07C-D04E1987CF9D,BootID:0a8fe774-fb14-491c-9538-7f647358d372,KernelVersion:4.13.0-1018-azure,OSImage:Ubuntu 16.04.4 LTS,ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.12.0-alpha.0,KubeProxyVersion:v1.12.0-alpha.0,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcrio.azureedge.net/google_containers/hyperkube-amd64@sha256:b92fc22f40fa4bf5d72ddd6e82ffd5d5a98ac9201eeb086843899f3a9fbf3eee gcrio.azureedge.net/google_containers/hyperkube-amd64:v1.12.0-alpha.0] 619581352} {[k8s.gcr.io/nginx-slim-amd64@sha256:6654db6d4028756062edac466454ee5c9cf9b20ef79e35a81e3c840031eb1e2b k8s.gcr.io/nginx-slim-amd64:0.20] 103591055} {[k8s.gcr.io/nginx-slim-amd64@sha256:8ca6a9ecef3b2ef02f6e0c3d449235d9c53d532f420cc0a29a6a133aa88df256 k8s.gcr.io/nginx-slim-amd64:0.21] 95339966} {[gcr.io/kubernetes-e2e-test-images/nettest-amd64@sha256:ff598458029b42e23b823a3a690c07e1f6921627f3fc49d007033494eca13141 gcr.io/kubernetes-e2e-test-images/nettest-amd64:1.0] 30381916} {[gcr.io/kubernetes-e2e-test-images/hostexec-amd64@sha256:bdaecec5adfa7c79e9525c0992fdab36c2d68066f5e91eff0d1d9e8d73c654ea gcr.io/kubernetes-e2e-test-images/hostexec-amd64:1.1] 8407119} {[gcr.io/kubernetes-e2e-test-images/netexec-amd64@sha256:2edfad424a541b9e024f26368d3a5b7dcc1d7cd27a4ee8c1d8c3f81d9209ab2e gcr.io/kubernetes-e2e-test-images/netexec-amd64:1.0] 6227659} {[gcr.io/kubernetes-e2e-test-images/redis-amd64@sha256:3e01bcaf67cb9b5c9fa7f57ba92539c8962d59c9647b91e9ec5047a89e2bc49a gcr.io/kubernetes-e2e-test-images/redis-amd64:1.0] 5850779} {[gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64@sha256:2dd4032e98a0450d95a0ac71a5e465f542a900812d8c41bc6ca635aed1a5fc91 gcr.io/kubernetes-e2e-test-images/serve-hostname-amd64:1.0] 5470001} {[gcr.io/kubernetes-e2e-test-images/nautilus-amd64@sha256:a4e859e40750d0e1a94470445bea3bc0ba4edc7363863a0be7a3714336040daa gcr.io/kubernetes-e2e-test-images/nautilus-amd64:1.0] 4415165} {[gcr.io/kubernetes-e2e-test-images/kitten-amd64@sha256:36e05bc3dfe1cd0f2d2d119f47fbb6e1c75037b291073012d57fbf28af3de3d7 gcr.io/kubernetes-e2e-test-images/kitten-amd64:1.0] 4408701} {[gcr.io/kubernetes-e2e-test-images/porter-amd64@sha256:b5923cab026ae1a96325823e10254c9beda240126355a20ca64b59445ec631bb gcr.io/kubernetes-e2e-test-images/porter-amd64:1.0] 4400998} {[gcr.io/kubernetes-e2e-test-images/test-webserver-amd64@sha256:cd237408ae94e22c4f5c6d7c6f56708341db6c428180fe1fe011c17bf9d03c50 gcr.io/kubernetes-e2e-test-images/test-webserver-amd64:1.0] 4393904} {[gcr.io/kubernetes-e2e-test-images/liveness-amd64@sha256:5f89fb4e972426ae8d1d60dcf041f631f4d79922f50cc7ecbac2a8d1ed66edae gcr.io/kubernetes-e2e-test-images/liveness-amd64:1.0] 4279645} {[gcr.io/kubernetes-e2e-test-images/entrypoint-tester-amd64@sha256:ed08ffff86b0a2016fa650ff76ea4b85fc3dfcab30ee4b7bd47d514ab765da89 gcr.io/kubernetes-e2e-test-images/entrypoint-tester-amd64:1.0] 2276578} {[gcr.io/kubernetes-e2e-test-images/mounttest-user-amd64@sha256:dda6519a95c934b46731a6b1492fed1b48ccc6d4aed4b754a46d7de8063a3e2b gcr.io/kubernetes-e2e-test-images/mounttest-user-amd64:1.0] 1450451} {[gcr.io/kubernetes-e2e-test-images/mounttest-amd64@sha256:dc4e2dcfbde16249c4662de673295d00778577bc2e2ca7013a1b85d4f47398ca gcr.io/kubernetes-e2e-test-images/mounttest-amd64:1.0] 1450451} {[busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47 busybox:latest] 1146369} {[k8s-gcrio.azureedge.net/pause-amd64@sha256:59eec8837a4d942cc19a52b8c09ea75121acc38114a2c68b98983ce9356b8610 k8s.gcr.io/pause@sha256:f78411e19d84a252e53bff71a4407a5686c46983a2c2eeed83929b888179acea k8s-gcrio.azureedge.net/pause-amd64:3.1 k8s.gcr.io/pause:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jun 13 03:19:13.043: INFO: &#xA;Logging kubelet events for node k8s-agentpool1-31296431-1&#xA;Jun 13 03:19:13.047: INFO: &#xA;Logging pods the kubelet thinks is on node k8s-agentpool1-31296431-1&#xA;Jun 13 03:19:13.055: INFO: pod-configmaps-8adf423c-6eb8-11e8-a8e2-000d3a184d46 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jun 13 03:19:13.055: INFO: ss-2 started at 2018-06-13 03:19:05 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container nginx ready: true, restart count 0&#xA;Jun 13 03:19:13.055: INFO: liveness-exec started at 2018-06-13 03:15:41 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container liveness ready: true, restart count 0&#xA;Jun 13 03:19:13.055: INFO: labelsupdate854888ee-6eb8-11e8-8768-000d3a184d46 started at 2018-06-13 03:19:02 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container client-container ready: true, restart count 0&#xA;Jun 13 03:19:13.055: INFO: pod-projected-configmaps-64e93643-6eb8-11e8-b5c9-000d3a184d46 started at 2018-06-13 03:18:08 +0000 UTC (0+3 container statuses recorded)&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container createcm-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container delcm-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container updcm-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:13.055: INFO: netserver-1 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jun 13 03:19:13.055: INFO: kube-proxy-fr77g started at 2018-06-13 02:57:26 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container kube-proxy ready: true, restart count 0&#xA;Jun 13 03:19:13.055: INFO: frontend-77c5d665c7-5fkvl started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jun 13 03:19:13.055: INFO: pod-secrets-727f66e9-6eb8-11e8-addd-000d3a184d46 started at 2018-06-13 03:18:30 +0000 UTC (0+3 container statuses recorded)&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container creates-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container dels-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container upds-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:13.055: INFO: redis-slave-597bcc5997-k94w4 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jun 13 03:19:13.055: INFO: ss-0 started at 2018-06-13 03:18:44 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container nginx ready: false, restart count 0&#xA;Jun 13 03:19:13.055: INFO: pod-projected-secrets-8070ab60-6eb8-11e8-8891-000d3a184d46 started at 2018-06-13 03:18:54 +0000 UTC (0+3 container statuses recorded)&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container creates-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container dels-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container upds-volume-test ready: true, restart count 0&#xA;Jun 13 03:19:13.055: INFO: liveness-http started at 2018-06-13 03:16:08 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.055: INFO: &#x9;Container liveness ready: true, restart count 0&#xA;Jun 13 03:19:13.122: INFO: &#xA;Latency metrics for node k8s-agentpool1-31296431-1&#xA;Jun 13 03:19:13.122: INFO: {Operation: Method:pod_start_latency_microseconds Quantile:0.99 Latency:13.88936s}&#xA;Jun 13 03:19:13.122: INFO: &#xA;Logging node info for node k8s-master-31296431-0&#xA;Jun 13 03:19:13.132: INFO: Node Info: &amp;Node{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:k8s-master-31296431-0,GenerateName:,Namespace:,SelfLink:/api/v1/nodes/k8s-master-31296431-0,UID:74e2fb02-6eb5-11e8-9654-000d3a134c0e,ResourceVersion:10344,Generation:0,CreationTimestamp:2018-06-13 02:57:06 +0000 UTC,DeletionTimestamp:&lt;nil&gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{beta.kubernetes.io/arch: amd64,beta.kubernetes.io/instance-type: Standard_F2,beta.kubernetes.io/os: linux,failure-domain.beta.kubernetes.io/region: eastus,failure-domain.beta.kubernetes.io/zone: 0,kubernetes.azure.com/cluster: t-xinhli-new3,kubernetes.io/hostname: k8s-master-31296431-0,kubernetes.io/role: master,},Annotations:map[string]string{node.alpha.kubernetes.io/ttl: 0,volumes.kubernetes.io/controller-managed-attach-detach: true,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Spec:NodeSpec{PodCIDR:10.244.0.0/24,DoNotUse_ExternalID:,ProviderID:azure:///subscriptions/c4528d9e-c99a-48bb-b12d-fde2176a43b8/resourceGroups/t-xinhli-new3/providers/Microsoft.Compute/virtualMachines/k8s-master-31296431-0,Unschedulable:false,Taints:[{node-role.kubernetes.io/master true NoSchedule &lt;nil&gt;}],ConfigSource:nil,},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{31158935552 0} {&lt;nil&gt;}  BinarySI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4114132992 0} {&lt;nil&gt;} 4017708Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {&lt;nil&gt;} 2 DecimalSI},ephemeral-storage: {{28043041951 0} {&lt;nil&gt;} 28043041951 DecimalSI},hugepages-1Gi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},hugepages-2Mi: {{0 0} {&lt;nil&gt;} 0 DecimalSI},memory: {{4009275392 0} {&lt;nil&gt;} 3915308Ki BinarySI},pods: {{110 0} {&lt;nil&gt;} 110 DecimalSI},},Phase:,Conditions:[{NetworkUnavailable False 2018-06-13 02:59:10 +0000 UTC 2018-06-13 02:59:10 +0000 UTC RouteCreated RouteController created a route} {OutOfDisk False 2018-06-13 03:19:07 +0000 UTC 2018-06-13 02:56:57 +0000 UTC KubeletHasSufficientDisk kubelet has sufficient disk space available} {MemoryPressure False 2018-06-13 03:19:07 +0000 UTC 2018-06-13 02:56:57 +0000 UTC KubeletHasSufficientMemory kubelet has sufficient memory available} {DiskPressure False 2018-06-13 03:19:07 +0000 UTC 2018-06-13 02:56:57 +0000 UTC KubeletHasNoDiskPressure kubelet has no disk pressure} {PIDPressure False 2018-06-13 03:19:07 +0000 UTC 2018-06-13 02:56:57 +0000 UTC KubeletHasSufficientPID kubelet has sufficient PID available} {Ready True 2018-06-13 03:19:07 +0000 UTC 2018-06-13 02:57:36 +0000 UTC KubeletReady kubelet is posting ready status. AppArmor enabled}],Addresses:[{InternalIP 10.240.255.5} {Hostname k8s-master-31296431-0}],DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:55cfce2841774bb4acf59a2b85bb2c17,SystemUUID:5E9AA3FE-6AB1-FF4E-8B54-9A53FE5E1C1C,BootID:ab62f162-195c-4fcc-87c7-9dd1fbb45e4b,KernelVersion:4.13.0-1018-azure,OSImage:Ubuntu 16.04.4 LTS,ContainerRuntimeVersion:docker://1.13.1,KubeletVersion:v1.12.0-alpha.0,KubeProxyVersion:v1.12.0-alpha.0,OperatingSystem:linux,Architecture:amd64,},Images:[{[gcrio.azureedge.net/google_containers/hyperkube-amd64@sha256:b92fc22f40fa4bf5d72ddd6e82ffd5d5a98ac9201eeb086843899f3a9fbf3eee gcrio.azureedge.net/google_containers/hyperkube-amd64:v1.12.0-alpha.0] 619581352} {[fseldow/azure-cloud-controller-manager@sha256:8e41588ea607b1104fd0d785a17f9694410710c09419a455b91da8053985d7df fseldow/azure-cloud-controller-manager:e065e27] 168173406} {[k8s-gcrio.azureedge.net/kube-addon-manager-amd64@sha256:3519273916ba45cfc9b318448d4629819cb5fbccbb0822cce054dd8c1f68cb60 k8s-gcrio.azureedge.net/kube-addon-manager-amd64:v8.6] 78384272} {[k8s-gcrio.azureedge.net/pause-amd64@sha256:59eec8837a4d942cc19a52b8c09ea75121acc38114a2c68b98983ce9356b8610 k8s-gcrio.azureedge.net/pause-amd64:3.1] 742472}],VolumesInUse:[],VolumesAttached:[],Config:nil,},}&#xA;Jun 13 03:19:13.132: INFO: &#xA;Logging kubelet events for node k8s-master-31296431-0&#xA;Jun 13 03:19:13.137: INFO: &#xA;Logging pods the kubelet thinks is on node k8s-master-31296431-0&#xA;Jun 13 03:19:13.156: INFO: kube-controller-manager-k8s-master-31296431-0 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jun 13 03:19:13.156: INFO: kube-proxy-5976x started at 2018-06-13 02:57:26 +0000 UTC (0+1 container statuses recorded)&#xA;Jun 13 03:19:13.156: INFO: &#x9;Container kube-proxy ready: true, restart count 0&#xA;Jun 13 03:19:13.156: INFO: kube-scheduler-k8s-master-31296431-0 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jun 13 03:19:13.156: INFO: cloud-controller-manager-k8s-master-31296431-0 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jun 13 03:19:13.156: INFO: kube-addon-manager-k8s-master-31296431-0 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jun 13 03:19:13.156: INFO: kube-apiserver-k8s-master-31296431-0 started at &lt;nil&gt; (0+0 container statuses recorded)&#xA;Jun 13 03:19:13.211: INFO: &#xA;Latency metrics for node k8s-master-31296431-0&#xA;�[1mSTEP�[0m: Dumping a list of prepulled images on each node to file /home/t-xinhli/new-provider/cloud-provider-azure/t-xinhli-new3/report/image-puller.txt&#xA;Jun 13 03:19:13.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready&#xA;�[1mSTEP�[0m: Destroying namespace &#34;e2e-tests-sched-pred-28bkn&#34; for this suite.&#xA;Jun 13 03:19:19.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered&#xA;Jun 13 03:19:19.348: INFO: namespace: e2e-tests-sched-pred-28bkn, resource: bindings, ignored listing per whitelist&#xA;Jun 13 03:19:19.376: INFO: namespace e2e-tests-sched-pred-28bkn deletion completed in 6.154572253s&#xA;[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]&#xA;  /home/t-xinhli/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70&#xA;</system-out>
      </testcase>
      <testcase name="[sig-api-machinery] AdmissionWebhook Should mutate pod and apply defaults after mutation" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] Subpath [Volume type: hostPathSymlink] should fail if subpath file is outside the volume [Slow]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap should be consumable from pods in volume as non-root with FSGroup [NodeFeature:FSGroup]" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set same fsGroup for two pods simultaneously" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-cli] Kubectl client [k8s.io] Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]" classname="Kubernetes e2e suite" time="20.663322767"></testcase>
      <testcase name="[sig-apps] stateful Upgrade [Feature:StatefulUpgrade] [k8s.io] stateful upgrade should maintain a functioning cluster" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-network] Services should prevent NodePort collisions" classname="Kubernetes e2e suite" time="0">
          <skipped></skipped>
      </testcase>
      <testcase name="[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]" classname="Kubernetes e2e suite" time="44.528940132"></testcase>
  </testsuite>